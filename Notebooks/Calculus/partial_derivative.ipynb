{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- By the end of this session, you are able to take derivative of a function over one variable\n",
    "\n",
    "- Also, you would enable to take partial derivative of a function over all of its variables \n",
    "\n",
    "- Find the minimum of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to differentiation\n",
    "\n",
    "- Differentiation is a technique used to calculate the slope of a graph at different points.\n",
    "\n",
    "<img src=\"diff_y_x2.png\" width=\"600\" height=\"600\">\n",
    "<img src=\"diff_y_x2_gragh.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the gradient of a function at certain point by definition\n",
    "\n",
    "- Choose small $\\Delta x$\n",
    "\n",
    "- $f^\\prime(x_0) = \\frac{f(x_0 + \\Delta x) - f(x_0)}{\\Delta x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Write a Python Code that calculates the gradient of $x^2$ at $x_0 = 3$ and $x_0 = -2$ from above definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.000001000927568\n",
      "-3.999998999582033\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "\n",
    "eps = 1e-6\n",
    "x = 3\n",
    "print((f(x + eps) - f(x)) / eps)\n",
    "x = -2\n",
    "print((f(x + eps) - f(x)) / eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative Table\n",
    "\n",
    "- https://www.qc.edu.hk/math/Resource/AL/Derivative%20Table.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend Gradient into Two-Dimensional Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets watch this video about Partial Derivative Intro from Khan Academy: https://www.youtube.com/watch?v=AXqhWeUEtQU&t=175s\n",
    "\n",
    "\n",
    "- Consider the function $f(x, y) = x^2/y$. Calculate the first order\n",
    "partial derivatives ($\\partial f/\\partial x$ and $\\partial f/\\partial y$) and evaluate them at the point P(2, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can use Symbolic Python package (libarary) to compute the derivatives and partial derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*x/y\n",
      "-x**2/y**2\n",
      "4.00000000000000\n",
      "-4.00000000000000\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, diff\n",
    "x, y = symbols('x y', real=True)\n",
    "f = (x**2)/y\n",
    "fx = diff(f, x, evaluate=True)\n",
    "fy = diff(f, y, evaluate=True)\n",
    "print(fx)\n",
    "print(fy)\n",
    "# print(f.evalf(subs={x: 2, y: 1}))\n",
    "print(fx.evalf(subs={x: 2, y: 1}))\n",
    "print(fy.evalf(subs={x: 2, y: 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Reading: Tensorflow is a powerful package from Google that calculate the derivatives and partial derivatives numerically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(1.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    z = tf.divide(tf.multiply(x, x), y)\n",
    "\n",
    "# Use the tape to compute the derivative of z with respect to the\n",
    "# intermediate value x and y.\n",
    "dz_dx = t.gradient(z, x)\n",
    "dz_dy = t.gradient(z, y)\n",
    "\n",
    "\n",
    "print(dz_dx)\n",
    "print(dz_dy)\n",
    "\n",
    "# All at once:\n",
    "gradients = t.gradient(z, [x, y])\n",
    "print(gradients)\n",
    "\n",
    "\n",
    "del t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Reading: When x and y declared as constant, should add t.watch(x) and t.watch(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "x = tf.constant(2.0)\n",
    "y = tf.constant(1.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    t.watch(x)\n",
    "    t.watch(y)\n",
    "    z = tf.divide(tf.multiply(x, x), y)\n",
    "\n",
    "# Use the tape to compute the derivative of z with respect to the\n",
    "# intermediate value y.\n",
    "dz_dx = t.gradient(z, x)\n",
    "dz_dy = t.gradient(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Partial Derivative from Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0000010006480125\n",
      "-3.9999959997594203\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    return x**2/y\n",
    "\n",
    "\n",
    "eps = 1e-6\n",
    "x = 2\n",
    "y = 1\n",
    "print((f(x + eps, y) - f(x, y)) / eps)\n",
    "print((f(x, y + eps) - f(x, y)) / eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks about right! This works rather well and it is trivial to implement, but it is just\n",
    "an approximation, and importantly you need to call f() at least once per parameter\n",
    "(not twice, since we could compute f(x, y) just once). This makes this approach\n",
    "intractable for large systems (for example neural networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Do we need partial Gradients\n",
    "\n",
    "- In many applications, more specifically DS applications, we want to find the Minimum of a cost function\n",
    "\n",
    "- Most of the time, cost function is the system error and we want to have minimum error\n",
    "\n",
    "<img src=\"gradient_descent.png\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding minimum of a function\n",
    "\n",
    "- Assume we want to minimize the function $J$ which has two variables $w_0$ and $w_1$\n",
    "\n",
    "- Two options to find the minimum of $J(w_0, w_1)$:\n",
    "\n",
    "    - Take partial derivatives of $J(w_0, w_1)$ w.r.t. $w_0$ and $w_1$ -> $\\partial J(w_0, w_1)/\\partial w_0$ and $\\partial J(w_0, w_1)/\\partial w_1$ and put them to zero -> $\\partial J(w_0, w_1)/\\partial w_0 = 0$ and $\\partial J(w_0, w_1)/\\partial w_1 = 0$. In this approach we should solve system of linear or non-linear equation\n",
    "    \n",
    "    - Use Gradient Descent algorithm. It means define step-size $\\alpha$ as a small number and arbitrary random initial value for $w_0 = np.random.randn()$ and $w_1 = np.random.randn()$. Then update the $w_0$ and $w_1$ by:\n",
    "    \n",
    "        $w_0 = w_0 - \\alpha \\partial J(w_0, w_1)/\\partial w_0$\n",
    "        \n",
    "        $w_1 = w_1 - \\alpha \\partial J(w_0, w_1)/\\partial w_1$\n",
    "        \n",
    "    In `for loop`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
