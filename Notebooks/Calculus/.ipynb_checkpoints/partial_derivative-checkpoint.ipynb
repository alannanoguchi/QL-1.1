{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
<<<<<<< HEAD
    "- By the end of this session, you are able to take derivative of a function over one variable\n",
    "\n",
    "- Also, you would enable to take partial derivative of a function over all of its variables \n",
    "\n",
    "- Find the minimum of the function"
=======
    "By the end of this session you should be able to...\n",
    "\n",
    "1. Take the derivative of a function over one variable\n",
    "1. Take the partial derivative of a function over all of its variables \n",
    "1. Find the minimum of the function to obtain the best line that represents relationships between two variables in a dataset"
>>>>>>> e9cd2a9add8561f07482de3e9c3823450db9cf65
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Introduction to differentiation\n",
    "\n",
    "- Differentiation is a technique used to calculate the slope of a graph at different points.\n",
=======
    "## Why are derivatives important?\n",
    "\n",
    "Derivatives are the foundation for Linear Regression (a topic we'll cover later in the course) that allows us to obtain the best line that represents relationships between two variables in a dataset.\n",
    "\n",
    "## Introduction to Derivatives\n",
    "\n",
    "The process of fidning a derivative is called **Differentiation**, which is a technique used to calculate the slope of a graph at different points.\n",
    "\n",
    "### Activity - Derivative Tutorial:\n",
    "\n",
    "1. Go through this [Derivative tutorial from Math Is Fun](https://www.mathsisfun.com/calculus/derivatives-introduction.html) (15 min)\n",
    "1. When you're done, talk with a partner about topics you still have questions on. See if you can answer each other's questions. (5 min)\n",
    "1. We'll then go over questions on the tutorial as a class (10 min)\n",
    "\n",
    "### Review Diagram\n",
    "\n",
    "Review the below diagram as a class, and compare with what you just learned in the above Derivative Tutorial. Note that a Gradient Function is just another name for the Derivative of a function:\n",
>>>>>>> e9cd2a9add8561f07482de3e9c3823450db9cf65
    "\n",
    "<img src=\"diff_y_x2.png\" width=\"600\" height=\"600\">\n",
    "<img src=\"diff_y_x2_gragh.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Calculate the gradient of a function at certain point by definition\n",
    "\n",
    "- Choose small $\\Delta x$\n",
    "\n",
    "- $f^\\prime(x_0) = \\frac{f(x_0 + \\Delta x) - f(x_0)}{\\Delta x}$"
=======
    "## Derivative Formula\n",
    "\n",
    "- Choose small $\\Delta x$\n",
    "\n",
    "- $f^\\prime(x) = \\frac{d}{dx}f(x) = \\frac{\\Delta x}{\\Delta y}  = \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}$\n",
    "\n",
    "Remember that $\\Delta x$ approaches 0. So if plugging in a value in the above formula, choose a _very_ small number, or simplify the equation further such that all $\\Delta x = 0$, like we saw in the tutorial"
>>>>>>> e9cd2a9add8561f07482de3e9c3823450db9cf65
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Activity: Write a Python Code that calculates the gradient of $x^2$ at $x_0 = 3$ and $x_0 = -2$ from above definition"
=======
    "## Activity: Write a Python function that calculates the gradient of $x^2$ at $x = 3$ and $x = -2$ using the above definition"
>>>>>>> e9cd2a9add8561f07482de3e9c3823450db9cf65
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.000001000927568\n",
      "-3.999998999582033\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "\n",
    "eps = 1e-6\n",
    "x = 3\n",
    "print((f(x + eps) - f(x)) / eps)\n",
    "x = -2\n",
    "print((f(x + eps) - f(x)) / eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Derivative Table\n",
    "\n",
=======
    "Note that these values match $2x$, our derivative of $x^2$:\n",
    "\n",
    "$2*3 = 6$\n",
    "\n",
    "$2 * -2 = -4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative Table\n",
    "\n",
    "As a shortcut, use the second page of this PDF to find the derivative for common formulas. Utilize this as a resource going forward!\n",
    "\n",
>>>>>>> e9cd2a9add8561f07482de3e9c3823450db9cf65
    "- https://www.qc.edu.hk/math/Resource/AL/Derivative%20Table.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend Gradient into Two-Dimensional Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "- Lets watch this video about Partial Derivative Intro from Khan Academy: https://www.youtube.com/watch?v=AXqhWeUEtQU&t=175s\n",
    "\n",
    "\n",
    "- Consider the function $f(x, y) = x^2/y$. Calculate the first order\n",
    "partial derivatives ($\\partial f/\\partial x$ and $\\partial f/\\partial y$) and evaluate them at the point P(2, 1)."
=======
    "Now we know how to calculate a derivative of one variable. But what if we have two?\n",
    "\n",
    "To do this, we need to utilize **Partial Derivatives**. Calculating a partial derivative is essentially calculating two derivatives for a function: one for each variable, where they other variable is set to a constant.\n",
    "\n",
    "### Activity - Partial Derivative Video\n",
    "\n",
    "Lets watch this video about Partial Derivative Intro from Khan Academy: https://youtu.be/AXqhWeUEtQU\n",
    "\n",
    "**Note:** Here are some derivative shortcuts that will help in the video:\n",
    "\n",
    "$\\frac{d}{dx}x^2 = 2x$\n",
    "\n",
    "$\\frac{d}{x}sin(x) = cos(x)$\n",
    "\n",
    "$\\frac{d}{dx}x = 1$\n",
    "\n",
    "### Activity - Now You Try!\n",
    "Consider the function $f(x, y) = \\frac{x^2}{y}$\n",
    "\n",
    "- Calculate the first order partial derivatives ($\\frac{\\partial f}{\\partial x}$ and $\\frac{\\partial f}{\\partial y}$) and evaluate them at the point $P(2, 1)$."
>>>>>>> e9cd2a9add8561f07482de3e9c3823450db9cf65
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## We can use Symbolic Python package (libarary) to compute the derivatives and partial derivatives"
=======
    "## We can use the Symbolic Python package (library) to compute the derivatives and partial derivatives"
>>>>>>> e9cd2a9add8561f07482de3e9c3823450db9cf65
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*x/y\n",
      "-x**2/y**2\n",
      "4.00000000000000\n",
      "-4.00000000000000\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, diff\n",
<<<<<<< HEAD
    "x, y = symbols('x y', real=True)\n",
    "f = (x**2)/y\n",
=======
    "# initialize x and y to be symbols to use in a function\n",
    "x, y = symbols('x y', real=True)\n",
    "f = (x**2)/y\n",
    "# Find the partial derivatives of x and y\n",
>>>>>>> e9cd2a9add8561f07482de3e9c3823450db9cf65
    "fx = diff(f, x, evaluate=True)\n",
    "fy = diff(f, y, evaluate=True)\n",
    "print(fx)\n",
    "print(fy)\n",
    "# print(f.evalf(subs={x: 2, y: 1}))\n",
    "print(fx.evalf(subs={x: 2, y: 1}))\n",
    "print(fy.evalf(subs={x: 2, y: 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Optional Reading: Tensorflow is a powerful package from Google that calculate the derivatives and partial derivatives numerically "
=======
    "## Optional Reading: Tensorflow is a powerful package from Google that calculates the derivatives and partial derivatives numerically "
>>>>>>> e9cd2a9add8561f07482de3e9c3823450db9cf65
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(1.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    z = tf.divide(tf.multiply(x, x), y)\n",
    "\n",
    "# Use the tape to compute the derivative of z with respect to the\n",
    "# intermediate value x and y.\n",
    "dz_dx = t.gradient(z, x)\n",
    "dz_dy = t.gradient(z, y)\n",
    "\n",
    "\n",
    "print(dz_dx)\n",
    "print(dz_dy)\n",
    "\n",
    "# All at once:\n",
    "gradients = t.gradient(z, [x, y])\n",
    "print(gradients)\n",
    "\n",
    "\n",
    "del t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Optional Reading: When x and y declared as constant, should add t.watch(x) and t.watch(y)"
=======
    "## Optional Reading: When x and y are declared as constant, we should add `t.watch(x)` and `t.watch(y)`"
>>>>>>> e9cd2a9add8561f07482de3e9c3823450db9cf65
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "x = tf.constant(2.0)\n",
    "y = tf.constant(1.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    t.watch(x)\n",
    "    t.watch(y)\n",
    "    z = tf.divide(tf.multiply(x, x), y)\n",
    "\n",
    "# Use the tape to compute the derivative of z with respect to the\n",
    "# intermediate value y.\n",
    "dz_dx = t.gradient(z, x)\n",
    "dz_dy = t.gradient(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Partial Derivative from Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0000010006480125\n",
      "-3.9999959997594203\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    return x**2/y\n",
    "\n",
    "\n",
    "eps = 1e-6\n",
    "x = 2\n",
    "y = 1\n",
    "print((f(x + eps, y) - f(x, y)) / eps)\n",
    "print((f(x, y + eps) - f(x, y)) / eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Looks about right! This works rather well and it is trivial to implement, but it is just\n",
    "an approximation, and importantly you need to call f() at least once per parameter\n",
    "(not twice, since we could compute f(x, y) just once). This makes this approach\n",
    "intractable for large systems (for example neural networks)."
=======
    "Looks about right! This works rather well, but it is just an approximation. Also, you need to call `f()` at least once per parameter (not twice, since we could compute `f(x, y)` just once). This makes this approach difficult to control for large systems (for example neural networks)."
>>>>>>> e9cd2a9add8561f07482de3e9c3823450db9cf65
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Why Do we need partial Gradients\n",
    "\n",
    "- In many applications, more specifically DS applications, we want to find the Minimum of a cost function\n",
    "\n",
    "- Most of the time, cost function is the system error and we want to have minimum error\n",
    "\n",
    "<img src=\"gradient_descent.png\" width=\"800\" height=\"800\">"
=======
    "## Why Do we need Partial Gradients?\n",
    "\n",
    "In many applications, more specifically DS applications, we want to find the Minimum of a cost function\n",
    "\n",
    "- **Cost Function:** a function used in machine learning to help correct / change behaviour to minimize mistakes. Or in other words, a measure of how wrong the model is in terms of its ability to estimate the relationship between x and y. [Source](https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220)\n",
    "\n",
    "\n",
    "Why do we want to find the minimum for a cost function? Given that a cost function mearues how wrong a model is, we want to _minimize_ that error!\n",
    "\n",
    "In Machine Learning, we frequently use models to run our data through, and cost functions help us figure out how badly our models are performing. We want to find parameters (also known as **weights**) to minimize our cost function, therefore minimizing error!\n",
    "\n",
    "We find find these optimal weights by using a **Gradient Descent**, which is an algorithm that tries to find the minimum of a function (exactly what we needed!). The gradient descent tells the model which direction it should take in order to minimize errors, and it does this by selecting more and more optimal weights until we've minimized the function! We'll learn more about models when we talk about Linear Regression in a future lesson, but for now, let's review the Gradient Descent process with the below images, given weights $w_0$ and $w_1$:\n",
    "\n",
    "<img src=\"gradient_descent.png\" width=\"800\" height=\"800\">\n",
    "\n",
    "Look at that bottom right image. Looks like we're using partial derivatives to find out optimal weights. And we know exactly how to do that!"
>>>>>>> e9cd2a9add8561f07482de3e9c3823450db9cf65
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding minimum of a function\n",
    "\n",
<<<<<<< HEAD
    "- Assume we want to minimize the function $J$ which has two variables $w_0$ and $w_1$\n",
    "\n",
    "- Two options to find the minimum of $J(w_0, w_1)$:\n",
    "\n",
    "    - Take partial derivatives of $J(w_0, w_1)$ w.r.t. $w_0$ and $w_1$ -> $\\partial J(w_0, w_1)/\\partial w_0$ and $\\partial J(w_0, w_1)/\\partial w_1$ and put them to zero -> $\\partial J(w_0, w_1)/\\partial w_0 = 0$ and $\\partial J(w_0, w_1)/\\partial w_1 = 0$. In this approach we should solve system of linear or non-linear equation\n",
    "    \n",
    "    - Use Gradient Descent algorithm. It means define step-size $\\alpha$ as a small number and arbitrary random initial value for $w_0 = np.random.randn()$ and $w_1 = np.random.randn()$. Then update the $w_0$ and $w_1$ by:\n",
    "    \n",
    "        $w_0 = w_0 - \\alpha \\partial J(w_0, w_1)/\\partial w_0$\n",
    "        \n",
    "        $w_1 = w_1 - \\alpha \\partial J(w_0, w_1)/\\partial w_1$\n",
    "        \n",
    "    In `for loop`"
   ]
=======
    "Assume we want to minimize the function $J$ which has two weights $w_0$ and $w_1$\n",
    "\n",
    "We have two options to find the minimum of $J(w_0, w_1)$:\n",
    "\n",
    "1. Take partial derivatives of $J(w_0, w_1)$ with relation to $w_0$ and $w_1$:\n",
    "\n",
    "$\\frac{\\partial J(w_0, w_1)}{\\partial w_0}$\n",
    "\n",
    "$\\frac{\\partial J(w_0, w_1)}{\\partial w_1}$\n",
    "\n",
    "And find the appropriate weights such that the partial derivatives equal 0:\n",
    "\n",
    "$\\frac{\\partial J(w_0, w_1)}{\\partial w_0} = 0$\n",
    "\n",
    "$\\frac{\\partial J(w_0, w_1)}{\\partial w_1} = 0$\n",
    "\n",
    "In this approach we should solve system of linear or non-linear equation\n",
    "\n",
    "2. Use the Gradient Descent algorithm:\n",
    "\n",
    "First we need to define two things:\n",
    "\n",
    "- A step-size alpha ($\\alpha$) as a small number (like $1^(e-6)$ small)\n",
    "- An arbitrary random initial value for $w_0$ and $w_1$: $w_0 = np.random.randn()$ and $w_1 = np.random.randn()$\n",
    "\n",
    "Finally, we need to search for the most optimal $w_0$ and $w_1$ by using a loop to update the weights until we find the most optimal weights. We'll need to establish a threshold to compare weights to, toi know when to stop the loop. For example, if a weight from one iteration is within 0.0001 of the weight from the next iteration, we can stop the loop (0.0001 is our threshold here)\n",
    "\n",
    "Let's review some pseudocode for how to implement this algorithm:\n",
    "\n",
    "```\n",
    "initialize the following:\n",
    "    a starting weight value\n",
    "    the learning rate (alpha) (very small)\n",
    "    the threshold (small)\n",
    "    the current threshold (start at 1)\n",
    "    \n",
    "while the current threshold is less than the threshold:\n",
    "    store the current values of the weights into a previous value variable \n",
    "    set the weight values to new values based on the algorithm\n",
    "    set the current threshold from the difference of the current weight value and the previous weight value\n",
    "```\n",
    "\n",
    "How do we `set the weight values to new values based on the algorithm`? by using the below equations:\n",
    "    \n",
    "$w_0 = w_0 - \\alpha \\frac{\\partial J(w_0, w_1)}{\\partial w_0}$\n",
    "        \n",
    "$w_1 = w_1 - \\alpha \\frac{\\partial J(w_0, w_1)}{\\partial w_1}$\n",
    "\n",
    "Try to write the function yourself, creating real code from the pseudocode!\n",
    "\n",
    "**Stretch Challenge:** We may also want to limit the number of loops we do, in addition to checking the threshold. Determine how we may go about doing that\n",
    "\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Derivative tutorial from Math Is Fun](https://www.mathsisfun.com/calculus/derivatives-introduction.html) \n",
    "- [Derivative Table](https://www.qc.edu.hk/math/Resource/AL/Derivative%20Table.pdf)\n",
    "- [Khan Academy - Partial Derivatives video](https://www.youtube.com/watch?v=AXqhWeUEtQU&feature=youtu.be)\n",
    "- [Towards Data Science - Machine Learning Fundamentals: cost functions and gradient Descent](https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> e9cd2a9add8561f07482de3e9c3823450db9cf65
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.7.4"
=======
   "version": "3.7.5"
>>>>>>> e9cd2a9add8561f07482de3e9c3823450db9cf65
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
